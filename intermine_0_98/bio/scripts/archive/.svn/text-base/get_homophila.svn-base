#!/usr/bin/perl

# script to download the latest homophila release into a directory like
# homophila/2007-03-07
# the version used in the directory name is the version mentioned on the Homophila web page
# the script should be run in /shared/data
# modified by Philip North 20/03/07 to include parts of the process that were previously done manually

use strict;
use warnings;
use IO::All;

BEGIN {
  # find the lib directory by looking at the path to this script
  push (@INC, ($0 =~ m:(.*)/.*:)[0] . '/../../intermine/perl/lib/');
}
use InterMine::DataDownloader;

#Define ARGVs
my ($logdir,$logname,$home);

$logdir="/shared/data/download_logs/";
my $tempname="temp_log.txt";
my $shared_data="/shared/data";

if(@ARGV!=3){
	#die "Wrong number of ARGVs!\nget_all.sh should supply the log directory, temporary log name, and /shared/data/.\n";
	warn "no arguments passed, using default values\n";
	$logname = $tempname;
	$home = $shared_data;
}else{
	$logdir = $ARGV[0];
	$logname= $ARGV[1];
	$home = $ARGV[2];
}

#homophila source directories and files
my $homophila_server = "http://superfly.ucsd.edu/homophila";
my $homophila_data_file = "homophila_all.txt";
my $homophila_matches_file = "homophila_matches.txt";
my $homophila_diseases_file = "homophila_diseases.txt";
my $protein_ids_file = "protein_ids.txt";

#flymine destination directories and files
my $homophila_main_dir = "$home/homophila";
my $temp_dir = "$homophila_main_dir/temp";
my $temp_file = "$temp_dir/temp.txt";
my $file_to_get = "$homophila_server/$homophila_data_file.gz";
my $date = &convert_date();
my $data_dir = "$homophila_main_dir/$date";
my $data_file = "$data_dir/$homophila_data_file";

my ($updated,$version_buffer,$log_buffer);
#make sure that the necessary directories exist and download the file
&checkdir_exists($homophila_main_dir);
&checkdir_exists($temp_dir);
&http_download($file_to_get,"$temp_file.gz");
&unzip_dir($temp_dir);

#compare the files, create version directory if it is a new version or
#the current data link is missing
my $current_link = "$home/homophila/current";
my $old_file = "$current_link/homophila_all.txt";

if(&compare_files($old_file,$temp_file)==1){
	print "Keeping downloaded files.\n";
	&checkdir_exists($data_dir);
	rename($temp_file, $data_file) or die "Cannot rename $temp_file: $!";
	&make_link($date,$current_link);
	$updated = 1;
	
	#get the homophila version number from the webpage
	my $expression = qr/Version ([\d\.]+)/;
	my $version = &search_webpage($homophila_server,$expression);
	
	$log_buffer = "Homophila\nNew data available for version $version in $data_dir containing file(s):\n";
	$version_buffer = "Homophila\n$version $date\nHomophila: Human disease to Drosophila gene data set\nhttp://superfly.ucsd.edu/homophila/\n";
}else{	
	unlink $temp_file;
	print "Downloaded files deleted.\n";
	$data_dir = $current_link;
	$updated = 0;
	$log_buffer = "Homophila\nCurrent data ok\n\n";
        &write_log($log_buffer,$logdir,$logname);
        exit(0);
}

#create the diseases and matches file and the hash needed to query Entrez
my $matches = "$data_dir/$homophila_matches_file";
my $diseases = "$data_dir/$homophila_diseases_file";
my $prots = "$data_dir/$protein_ids_file";

#check that the files are needed
if( (-e $matches)&&(-e $diseases)&&(-e $prots) ){
	warn "Homophila dieseases and matches files already generated\n";
}else{
	#process the downloaded file
	open HOMOPHILA_DATA, "$data_file"
	  or die "can't open $data_file\n";

	my $line = <HOMOPHILA_DATA>;
	if ($line !~ /CG\tOMIM_ID/) {
	  die "expected first line of $data_file to be a header\n";
	}
	my %homophila_matches = ();
	my %homophila_diseases = ();
	my %proteinID = ();
	
	while ($line = <HOMOPHILA_DATA>) {
		chomp $line;
		my @bits = split /\t/, $line;
	
		#get unique matches and diseases
		$homophila_matches{join "\t", @bits[0..3]}++;
		if(@bits[1,6]){
			$homophila_diseases{join "\t", @bits[1,6]}++;
		}
		#and protein Ids
		$proteinID{$bits[2]} = $bits[2];
	}
	close HOMOPHILA_DATA;

	#write processed data to file
	print "Writing files.\n";
	&print_hash2file($matches,%homophila_matches);
	&print_hash2file($diseases,%homophila_diseases);
	&print_hash2file($prots,%proteinID);
	
	$log_buffer	.= "$homophila_matches_file\n$homophila_diseases_file\n";
}

#print file
sub print_hash2file{
    my ($filename, %hash) = @_;
    
    open FH, ">$filename"
        or die "can't open $filename\n";
    for my $line (sort keys %hash) {
        print FH "$line\n";
    }
    close FH;
}

#===================================================================================
# this part replaces the manual batch upload of protein ids to ncbi entrez
# uses eutils to access ncbi with the protein Ids and retrieve the GenPept entries.

use LWP::Simple;

# output file
my $sequences_file = "$data_dir/sequences.gp";

# eutils URL
my $utils = "http://www.ncbi.nlm.nih.gov/entrez/eutils";

# define database and output format
my $db     = "Protein";
my $report = "gp";

if(-e $sequences_file){
	warn "sequences.gp already generated\n";
}else{
	print "Querying NCBI Entrez and writing sequences.gp\n";
	# URL for esearch
	my $esearch = "$utils/esearch.fcgi?db=$db&retmax=1&usehistory=y&term=";
				  
	#open file handle for writing
	open SEQUENCES, ">$sequences_file"
	   or die "can't open $sequences_file\n";
	
	#open protein Ids file
	open(PROT, "<$prots") or die "$!";
	while (my $queryID = <PROT>) {
		chomp $queryID;
		# join id to be queried by esearch and final esearch URL, get the result
		#print "Query ID $queryID\n";
		my $esearch_result = get($esearch . $queryID);
	
		# identify parameters for efetch
		$esearch_result =~
		  m|<Count>(\d+)</Count>.*<QueryKey>(\d+)</QueryKey>.*<WebEnv>(\S+)</WebEnv>|s;

		my $Count    = $1;
		my $QueryKey = $2;
		my $WebEnv   = $3;
		print "Count = $Count; QueryKey = $QueryKey; WebEnv = $WebEnv\n\n";

		my $retstart;
		my $retmax=3;
	
		# for each record found by esearch, use efetch to retrieve it
		for($retstart = 0; $retstart < $Count; $retstart += $retmax) {
			my $efetch = "$utils/efetch.fcgi?rettype=$report&retmode=text&"
			          . "retstart=$retstart&retmax=$retmax&" 
			          . "db=$db&query_key=$QueryKey&WebEnv=$WebEnv";     
			my $efetch_result = get($efetch);
	  		print SEQUENCES "$efetch_result\n";		
		}
		#&delay();
	}close SEQUENCES;
}
# implements the three second delay between queries as required by ncbi
sub delay(){
    my $future = (time + 3);
    while (1) {
    	if (time >= $future) {
            return;        
        }
    }
}
#=================================================================================
# this part was once create_protein_gene.pl 

# output file
my $prot_gene_file = "$data_dir/protein_gene.txt";
my $more = 0;
my $cds = 0;

if(-e $prot_gene_file){
	warn "protein_gene.txt already generated\n";
}else{
	print "Writing protein_gene.txt\n";
	
	#write to...
	open PROT_GENES, ">$prot_gene_file"
	   or die "can't open $prot_gene_file\n";
	
	#read from the esearch results
	open(FILE, "<$sequences_file") or die "$!";
	while (<FILE>) {
		#find the first LOCUS and set the $more flag
		if (!$more && /LOCUS      /) {
	    	$more = 1;
	  	}
		#save the next gene acession number after the more flag is set
		#reset $more so that the identifier is ignored until another locus is found
		if( $more && (/(NP_\d+)/ || /(XP_\d+)/) ){
	    	print PROT_GENES "$1\t";
	    	$more = 0;
	    	$cds = 0;
		}
		#identify the CDS and set the $cds flag
		if (!$cds && /CDS      /) {
	    	$cds = 1;
  		}
		#save the gene name, set the $more flag for the next gene
		if ($cds && /gene="(.+)"/) {
    		print PROT_GENES "$1\n";
   			$more = 1;
  		}	
	}
	close (FILE) or die "$!";
	close PROT_GENES;
}
if($updated == 1){
	$log_buffer.="protein_gene.txt\n\n";
	&write_version($homophila_main_dir,$version_buffer);
	system "chmod -R a+r,g+w $data_dir";
}
&write_log($log_buffer,$logdir,$logname);


